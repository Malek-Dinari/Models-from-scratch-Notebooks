{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y44GuHycXA0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f22c69d-d54b-453e-eecc-50790ed39a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-03 22:28:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-11-03 22:28:17 (14.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dataset to train our model on. Download the tiny shakespeare dataset here :\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the tiny shakespeare dataset:\n",
        "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "aYSqbyWZI8j1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length (in characters) of dataset : \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJyDhBt0Jym5",
        "outputId": "f8725d8a-2399-45ec-cfa4-155bbed51d80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length (in characters) of dataset :  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# peeking at first 2500 characters :\n",
        "print(text[:2500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSj0YOOcJ9-c",
        "outputId": "144f36d3-bde9-4eef-a3be-f6331601c56c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First Citizen:\n",
            "Soft! who comes here?\n",
            "\n",
            "Second Citizen:\n",
            "Worthy Menenius Agrippa; one that hath always loved\n",
            "the people.\n",
            "\n",
            "First Citizen:\n",
            "He's one honest enough: would all the rest were so!\n",
            "\n",
            "MENENIUS:\n",
            "What work's, my countrymen, in hand? where go you\n",
            "With bats and clubs? The matter? speak, I pray you.\n",
            "\n",
            "First Citizen:\n",
            "Our business is not unknown to the senate; they have\n",
            "had inkling this fortnight what we intend to do,\n",
            "which now we'll show 'em in deeds. They say poor\n",
            "suitors have strong breaths: they shall k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all unique characters in text of our dataset tiny shakespeare :\n",
        "uchars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(uchars)\n",
        "\n",
        "print(''.join(uchars))\n",
        "print(f'nÂ° of unique chars : {vocab_size}')\n",
        "print('Y' in uchars)\n",
        "print(uchars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8LKkBDDKMy7",
        "outputId": "bd12dcf1-fec5-4189-bc67-a47ee679675e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "nÂ° of unique chars : 65\n",
            "True\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map characters (using unique characters: uchars) to integers\n",
        "stoi = { ch: i for i, ch in enumerate(uchars) }\n",
        "itos = { i: ch for i, ch in enumerate(uchars) }\n",
        "\n",
        "# encode /decode string of characters\n",
        "encode = lambda s: [stoi[c] for c in s ] # take string and encode it in list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "#use functions\n",
        "txt = \"Yo it's Malek !!\"\n",
        "print(encode(txt))\n",
        "print(decode(encode((txt))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkzr8fZIKzk-",
        "outputId": "6340c880-449d-43e7-d975-e11a49017266"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[37, 53, 1, 47, 58, 5, 57, 1, 25, 39, 50, 43, 49, 1, 2, 2]\n",
            "Yo it's Malek !!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the entire text dataset\n",
        "\n",
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype, data.type)\n",
        "print(data[:1500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmCR-pxcP4U6",
        "outputId": "fb09459d-d5c7-447e-e360-e3ba505c10cd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64 <built-in method type of Tensor object at 0x7e3d182cdee0>\n",
            "tensor([18, 47, 56,  ..., 58, 53,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train and validation sets from our dataset and for our training:\n",
        "n = int(0.875*len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "QQYZq7UTRrFH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretend we're sampling chunks of our training set randomly, we need to fix the maximum size of such random block\n",
        "block_size = 16\n",
        "\n",
        "print(train_data[:block_size * 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Owy3E7PSYX0",
        "outputId": "dc18c5bf-bb43-40f0-a792-327bef878f66"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "\n",
        "  print(f'When input is {context}, the target is {target}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfW56M1MTW6G",
        "outputId": "b8a30ef6-ad21-4505-de15-4a425e154bfb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]), the target is 47.\n",
            "When input is tensor([18, 47]), the target is 56.\n",
            "When input is tensor([18, 47, 56]), the target is 57.\n",
            "When input is tensor([18, 47, 56, 57]), the target is 58.\n",
            "When input is tensor([18, 47, 56, 57, 58]), the target is 1.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]), the target is 15.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), the target is 47.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]), the target is 64.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]), the target is 43.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]), the target is 52.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]), the target is 10.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]), the target is 0.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]), the target is 14.\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]), the target is 43.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # reproducibility seed (like 42 for statisictical analysis : PCA, LDA, PCR ....etc ) !\n",
        "\n",
        "batch_size = 4 # process independent sequences in parallel, just like for ANNs / CNNs\n",
        "block_size = 8 # maximum context length to do predictions\n",
        "\n",
        "def get_batch(data_split):\n",
        "\n",
        "  # generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if data_split == 'train' else val_data\n",
        "  idx = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i: i+block_size] for i in idx])\n",
        "  y = torch.stack([data[i+1: i+block_size+1] for i in idx])\n",
        "\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('Targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('------------------------------------------')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "  for t in range(block_size): # time dimension ?\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "\n",
        "    print(f'when input is {context.tolist()}, the target is : {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4aZDhfqWh8p",
        "outputId": "c0eb4b87-de9f-4aac-9432-b3ad90190638"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43,  6,  1, 40, 43, 57, 43, 43],\n",
            "        [57, 43,  1, 46, 59, 51, 57,  1],\n",
            "        [57,  6,  0, 17, 60, 43, 52,  1],\n",
            "        [ 1, 45, 47, 56, 50,  8,  0,  0]])\n",
            "Targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 6,  1, 40, 43, 57, 43, 43, 41],\n",
            "        [43,  1, 46, 59, 51, 57,  1, 39],\n",
            "        [ 6,  0, 17, 60, 43, 52,  1, 44],\n",
            "        [45, 47, 56, 50,  8,  0,  0, 22]])\n",
            "------------------------------------------\n",
            "when input is [43], the target is : 6\n",
            "when input is [43, 6], the target is : 1\n",
            "when input is [43, 6, 1], the target is : 40\n",
            "when input is [43, 6, 1, 40], the target is : 43\n",
            "when input is [43, 6, 1, 40, 43], the target is : 57\n",
            "when input is [43, 6, 1, 40, 43, 57], the target is : 43\n",
            "when input is [43, 6, 1, 40, 43, 57, 43], the target is : 43\n",
            "when input is [43, 6, 1, 40, 43, 57, 43, 43], the target is : 41\n",
            "when input is [57], the target is : 43\n",
            "when input is [57, 43], the target is : 1\n",
            "when input is [57, 43, 1], the target is : 46\n",
            "when input is [57, 43, 1, 46], the target is : 59\n",
            "when input is [57, 43, 1, 46, 59], the target is : 51\n",
            "when input is [57, 43, 1, 46, 59, 51], the target is : 57\n",
            "when input is [57, 43, 1, 46, 59, 51, 57], the target is : 1\n",
            "when input is [57, 43, 1, 46, 59, 51, 57, 1], the target is : 39\n",
            "when input is [57], the target is : 6\n",
            "when input is [57, 6], the target is : 0\n",
            "when input is [57, 6, 0], the target is : 17\n",
            "when input is [57, 6, 0, 17], the target is : 60\n",
            "when input is [57, 6, 0, 17, 60], the target is : 43\n",
            "when input is [57, 6, 0, 17, 60, 43], the target is : 52\n",
            "when input is [57, 6, 0, 17, 60, 43, 52], the target is : 1\n",
            "when input is [57, 6, 0, 17, 60, 43, 52, 1], the target is : 44\n",
            "when input is [1], the target is : 45\n",
            "when input is [1, 45], the target is : 47\n",
            "when input is [1, 45, 47], the target is : 56\n",
            "when input is [1, 45, 47, 56], the target is : 50\n",
            "when input is [1, 45, 47, 56, 50], the target is : 8\n",
            "when input is [1, 45, 47, 56, 50, 8], the target is : 0\n",
            "when input is [1, 45, 47, 56, 50, 8, 0], the target is : 0\n",
            "when input is [1, 45, 47, 56, 50, 8, 0, 0], the target is : 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # this is our input (embedding ?) to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14wuYtZOcmWU",
        "outputId": "60c416f2-a0e4-446a-d629-38364ef2f81d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[43,  6,  1, 40, 43, 57, 43, 43],\n",
            "        [57, 43,  1, 46, 59, 51, 57,  1],\n",
            "        [57,  6,  0, 17, 60, 43, 52,  1],\n",
            "        [ 1, 45, 47, 56, 50,  8,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # each token --directly---> reads off the logits, for the next token, from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    # ( idx, targets ) === (B,T) tensor of integers\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    # (B, T) === idx : array of indices in the current context\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      # get the predictions:\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      # last step only (main focus)\n",
        "      logits = logits[:, -1, :] # becomes (B, C) ?\n",
        "\n",
        "\n",
        "      # applying softmax : turn received logits into probablities\n",
        "      probs = F.softmax(logits,  dim=1) # (B,C)\n",
        "\n",
        "      # sampling from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "      # appending sampled index to running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upbGGvvvZJpE",
        "outputId": "07c9efde-29c4-48a0-824e-a6c965c21f9a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.4732, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create the optimizer that we are going to use using PyTorch:\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3 )"
      ],
      "metadata": {
        "id": "bYzh9BjlhZ1T"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "for steps in range(7500): # increate for better results ?\n",
        "\n",
        "  # sampling a batch of data:\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluating the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=None)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PySmgz0roTPt",
        "outputId": "d2f0c84b-791b-46c3-d40d-ceb00ab0a29e"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.46026873588562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating with our current model basically outputs random gibberish, we would need to train and optimize further to get to great results\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=3500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feupebUEpXQR",
        "outputId": "c4f35eb8-ba38-465b-8274-96a169860163"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POf imyos ar; l thim,\n",
            "AUCKtas\n",
            "PSindido neco hther?\n",
            "\n",
            "PUCI'd brd, al whayorend D t ay astad s me Caryot.\n",
            "\n",
            "Thanenoust rr, ws ce thateerd ak.\n",
            "NT:\n",
            "\n",
            "QULOWh hksshr aum, ct l: fawan?\n",
            "MI theatled\n",
            "I're gthuent mo igomadr\n",
            "I mm tin nigOM:\n",
            "thas, Pactefoughag mis.\n",
            "\n",
            "Buan'toundo Myonde swily myenan:\n",
            "Qwovise!\n",
            "TIARO, ble wer d OLBut ROUE angen git fou.\n",
            "Ists. cLES:\n",
            "GHeepes, VONCore SocBE:\n",
            "BULANLI t f, BUS wat t o inothere thoof d lor isuth mitenoorer itelazentarckeory a c'si' d he tesprogrr's by crthim acode fam tounoneou, JO, wats ivit ofodis$DI ha sor, willleZARGRLARIUS:\n",
            "Y:\n",
            "\n",
            "Thivitlainthine ken,\n",
            "\n",
            "erimy he aveco:\n",
            "RILie.\n",
            "Thofelly u RI:\n",
            "AKInfer:\n",
            "Thecouthals myotooupand my, luthir r hirot beat p, tend ins, y se.\n",
            "ghenarcllt y.\n",
            "HES:\n",
            "Iy orirl I imondareyo y ovethoves four it burengonond\n",
            "Were ante bencusamy wink houge s owhedor aver oras no a; llos O:\n",
            "Ant fishe\n",
            "Ang p-atho! ave ird, isaraoolaver, tharstur orpe Oru al wamy hr d t po mak.\n",
            "CEXESonoly d s! rd ieitee ithesi'ds then, hin bath are.\n",
            "Be athefesedie tofothis I: thill s no Ycothlillfesor tos whepe'dve ad\n",
            "onghellmavened theece\n",
            "Jast fieco IF th Ed hise CHar thinid dgllotciy.\n",
            "CAt frS:\n",
            "BOu ce tas,\n",
            "I'dime p.\n",
            "Platr nore S:\n",
            "Pqut, d by y I hes f sst bll undschandgo blls womy fe y he med bshiar it'de\n",
            "GA: chee RKENUTewechare loty d!\n",
            "\n",
            "SOnethain plo, ttheyo jofomeser ll thmoourd barchehainowrinmiengh th,\n",
            "I st wiventhoss?\n",
            "HOUS: wind s, spurer ceroll in sedo ul-sepuds eathet and, onond k.\n",
            "n deew pastent ally outhinto thy tese car'thavepe tol'd my t br' af ecowha s V:\n",
            "Thanthepigethitilds ter myo peashy s:\n",
            "Anghecangaveresldwice:MI sus thaime I be?\n",
            "Beysehy nour s:\n",
            "\n",
            "Ty methaper th;\n",
            "Toreesornd Andare che Bot manEOLIftor is nd t n'ire h ome ofe us, hodofinot tro nel, ous bourteas:\n",
            "PS:\n",
            "Ca busptr t\n",
            "LIUzen meorith,\n",
            "Bof t:\n",
            "LKENGJdvist, t t us lanait?\n",
            "Th penow nd l brdour, ancoune t acenE:\n",
            "PArad anyour so omase indeape, nd beyr withistew LENToedy e.\n",
            "Bo.\n",
            "\n",
            "Ole\n",
            "ARK:\n",
            "A ot ag, fashe bat'sloke cer s en: weZYo-mye ple oorat the,\n",
            "POLOLendan hand wathy:\n",
            "Th, oY e, -s:\n",
            "t,\n",
            "\n",
            "Bourloowsth Ifaititl hy heshifo RCO,f I:\n",
            "RDULisere g o msALA:\n",
            "By tifut d thepr!\n",
            "AUKINI ucowe.\n",
            "FI cke tiANTIInarsma w is, HEN heat ot irobe I PS: avope, myoboris heth\n",
            "Hotod an youchatre thepean o omatis t s:\n",
            "WAR:\n",
            "Sofe I isesealas f rdsthe aly thoumeit wamands at umZAnererdepras thepr'do d,\n",
            "Thin dere rnie p me!\n",
            "\n",
            "A:\n",
            "BUS:\n",
            "Anere; ifitithedson abeure I alewn s th.\n",
            "\n",
            "LA:\n",
            "ATh, Codin de hit bic.\n",
            "YBRowinor astread mol s sthe I iteen h te-memule imongrcoatay se:\n",
            "CUTe'lld mman'ecend, to blot GARK:\n",
            "Sches younghare s s orveameschenoun feevegeerveson, buleth she y sowetheshid adifuns; thincfSTEN fe itee r'eay--otofr y tay me; dalds yacour, hVand thadeap goloDudo theran! hewras surd He wiKIZAstorsotak withills!\n",
            "\n",
            "Ho! thencake s fesuerauck'seme hano pudirngas mearavirdo the; h'd minmyel ipar Stourst.\n",
            "F-mewheed, cothe!\n",
            "GXEist sow;\n",
            "CENUpedoegeremides brenee umince.\n",
            "Gorthasthil S:\n",
            "Buefas\n",
            "T:\n",
            "\n",
            "Wiend fey IOLay ndases mal t be s m d anse\n",
            "GARKINThes y tetordset at grinour.\n",
            "lon thend fous hers achedEdsoo hivese weas be ilare\n",
            "Se powheran\n",
            "t perret his, K:\n",
            "TENGout isiser prstthe at te chu by. s nnk'sin, f iso t m yourouthag, CUSponke ge war ave dou pern\n",
            "Tharersspoun, penoulla!\n",
            "A:\n",
            "My cif poover-eshem aver, fr crthe IONENIUELin t ge a ctio INAsth sucoks!\n",
            "etrFisend'd;\n",
            "CEnime e aireten t\n",
            "\n",
            "CHicof hl'sofofasChe w,\n",
            "LLourestousouee,\n",
            "Burnstjukne hath, be:\n",
            "wibre wiomaingonougio ourmend t I MI ve d the the IIOM:\n",
            "S: hoy mor ine'dwak imetor t oure no-prt!\n",
            "HAUpur mpumavas ithin,\n",
            "fe ikiurily:\n",
            "thangsea'?\n",
            "O:\n",
            "\n",
            "YObofowars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##  Mathematical trick in self-attention\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QVGryCF2rCb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weighted aggregation (research papers) : keywords : fuzzy logic/applied maths ??\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.tril(torch.ones((3, 3)))\n",
        "a = a / torch.sum(a, 1, keepdim=True) #\n",
        "b = torch.randint(0, 10, (3,2)).float()\n",
        "c = a @ b\n",
        "\n",
        "print('a=')\n",
        "print(a)\n",
        "print('-------')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('-------')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX3Bj3Pcs_zL",
        "outputId": "bf35fc8e-d405-4481-e99f-cba0c00a5f8b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "-------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "-------\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy example:\n",
        "# Version 1\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2 # batch, time, channels\n",
        "\n",
        "X = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYdC5ib9tgCi",
        "outputId": "4fdbc240-5ff0-42e0-9268-14b70f06ee1d"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  x[b,t] = mean_{i<=t} x[b,i]\n",
        "\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "\n",
        "    xprev = x[b ,:t+1] # (T,C)\n",
        "    xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "5-uCCGKFwksf"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 2 ==> we use matrix multiply for a weighted aggregation\n",
        "\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y88lbQfOQzTJ",
        "outputId": "1f4a3228-fec0-4563-a2df-eb8e7cb0a7d8"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 3 ==> with softmax\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=1)\n",
        "\n",
        "xbow3 = wei @ x\n",
        "\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9Zdwto1RXGT",
        "outputId": "ecc1d504-4c70-452d-a4e3-fde71e04355c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Version 4: NEW! Self-attention\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B,T,C = 4, 8, 32 # as usual: batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention!\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16 === C)\n",
        "q = query(x) # (B, T, 16 === C)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, T, 16) ---> (B, T, T)\n",
        "\n",
        "# wei = torch.zeros((T,T))\n",
        "#out = wei @ x\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_inVrZ7VaZR",
        "outputId": "416bbc06-eb2b-4ed7-8ee8-3ef566e18c8f"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzmZBkDwVuiQ",
        "outputId": "07e03013-f32e-4840-f910-78143a0edb92"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
              "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
              "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
              "        [-1.8044, -0.4126, -0.8306,  0.5899, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTCdTbWXYDiK",
        "outputId": "fff69957-f364-4fc4-e7b5-893e77bf0cab"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“âœï¸  Notes on Attention and GPT-like Architectures (GPT4o describing gpts ðŸ™)\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Attention as a **Communication Mechanism**\n",
        "- Attention acts as a way for different elements in a sequence (or \"nodes\") to **communicate** with each other.\n",
        "- You can imagine each part as a node in a **directed graph**. Each node receives information from others that \"point\" to it, aggregating this information with **data-dependent weights**.\n",
        "- Each node combines information using a **weighted sum** of inputs from other nodes.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. No Concept of Space in Attention\n",
        "- **Attention** itself has no understanding of token order or spatial relationships. It simply operates on a **set of vectors**.\n",
        "- To give the model positional information, we add **positional encodings** to the tokens so the model can understand each token's position in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Independence Across the Batch Dimension\n",
        "- Each example within a batch is processed **independently**.\n",
        "- Examples in the same batch never â€œtalkâ€ to each other or share information during the attention process.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Encoder vs. Decoder Attention Blocks\n",
        "- In an **encoder attention block**, tokens communicate freelyâ€”thereâ€™s no restriction on which tokens can attend to others.\n",
        "- In a **decoder attention block**, each token can only â€œseeâ€ previous tokens in the sequence. This restriction is applied using **triangular masking** (typically with `tril`).\n",
        "- This masking is crucial for **autoregressive models** (like GPT) where each token prediction depends only on past tokens, not future ones.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Self-Attention vs. Cross-Attention\n",
        "- **Self-Attention**: The **queries, keys, and values** all come from the **same source** (e.g., a sequence of tokens in a sentence). This allows the model to learn relationships within a single sequence.\n",
        "- **Cross-Attention**: The **queries** come from one source (like an input sequence), while the **keys and values** come from another (such as an encoder block in a different layer). This setup is often used in models where different parts handle distinct data types (e.g., in translation models).\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Scaled Attention\n",
        "- Attention weights (`wei`) are scaled by dividing by the square root of the vector size (`head_size`) before applying **softmax**.\n",
        "- This **scaling keeps gradients stable**: when the variance of `Q` and `K` is 1, this scaling keeps `wei` from concentrating too heavily on a few elements, ensuring softmax remains balanced.\n",
        "  \n",
        "---"
      ],
      "metadata": {
        "id": "ERuPpfPoawof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "gc4P4ya_YZsq"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l76rwCIb_7b",
        "outputId": "d3b2f7b4-6c72-4fe5-d3f9-37362d144d00"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3kdWP_qcBcE",
        "outputId": "c453273b-07bc-4866-d54d-65b009d9c035"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plfxbIkbcBeZ",
        "outputId": "636164aa-0655-4ad1-b7f3-19d163467d17"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, 0.6, -0.3, -0.3, -0.2, 0.45]), dim=-1) # dim === -1 ==> last dimension (for example dim=2 <---> dim=-1 if 2 is the last dimension of a tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZllzMqZcMub",
        "outputId": "0c59d231-1ef2-49ed-f19d-7a31879c3307"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1626, 0.2681, 0.1090, 0.1090, 0.1205, 0.2308])"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([-0.1, -0.16, 0.55, -0.2, 0.45])*8, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-qFuxnUfNSR",
        "outputId": "05943878-26e4-42ad-ac46-2b679bb1459b"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0038, 0.0023, 0.6846, 0.0017, 0.3076])"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, 0.16, 0.25, 0.2, 0.45])*8, dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAvcOgVed1zj",
        "outputId": "5c6e012f-f75b-49c0-9ce9-be3c03031327"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0406, 0.0657, 0.1349, 0.0904, 0.6683])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, 0.16, 0.25, 0.2, 0.45])*16, dim=-1) # sharpening softmax (multiply by ~big number that would just elevate one value) gets too peaky, converges to one-hot ðŸ’¡"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jh6ispxcvT-",
        "outputId": "e082b112-2d32-4996-c8fe-33ba67c45020"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.5843e-03, 9.3611e-03, 1.3254e-05, 1.7753e-02, 9.6929e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    #calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # # batch var\n",
        "    xhat = (x - xmean ) / torch.sqrt( xvar + self.eps)  # normalizes (to unit variance)\n",
        "    self.out = self.gamma * xhat + self.beta # LAYERNORM paper for formula + more details!\n",
        "    return self.out\n",
        "\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(200)\n",
        "x = torch.randn(32, 200) # batch size fo 200D (200-dimensional) vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoA7A24JcxWv",
        "outputId": "bc9ca253-df9c-4d45-a2f8-8dde5e280779"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,standard deviation (Ã©cart type) of one feature, across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOAweE6idjBM",
        "outputId": "d9f67b2e-1091-4447-8600-872383eb4df7"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1489), tensor(0.8685))"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean, std deviation of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrkUJ-nshi_1",
        "outputId": "34acc9c0-fdd2-4be8-d999-96cc68227bdb"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-3.5763e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English to French translation example:\n",
        "\n",
        "# <------------ ENCODE -----------><----------------- DECODE -------------------->\n",
        "# neural networks are awesome! <START> les rÃ©seaux de neurones sont gÃ©niaux! <END>"
      ],
      "metadata": {
        "id": "yHYU3asyhjBu"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# model's hyperparameters\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 6150\n",
        "eval_interval = 120\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0 # can add later\n",
        "# ----------------\n",
        "# ----------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# all the unique characters that occur in this text:\n",
        "uchars = sorted(list(set(text)))\n",
        "vocab_size = len(uchars)\n",
        "# create mapping (between) characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(uchars) }\n",
        "itos = { i:ch for i,ch in enumerate(uchars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string as input, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder : input is the list of integers, outputs a string (back)\n",
        "\n",
        "\n",
        "\n",
        "# Train and test sets/splits:\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.88*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# loading data\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  idx = torch.randint(len(data) - block_size,  (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad() # this should say: \"we don't intend to use backward() meaning no backprop (backpropagation) so no retaining of gradients values for model parameters needed\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "\n",
        "  for split in ['train', 'val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "          X, Y = get_batch(split)\n",
        "          logits, loss = model(X, Y)\n",
        "          losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)   # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "\n",
        "    # \"affinities\" or attention scores computation:\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B,T,C)\n",
        "    out = wei @ v # (B,T,T) @ (B,T,C) --> (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Multiple heads of self-attention in parallel\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "      out = self.dropout(self.proj(out))\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  simple linear layer followed by a non-linearity\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  \"\"\"\n",
        "  This is a Transformer block: communication followed by computation\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_embed, n_head):\n",
        "        # n_embed: embedding dimension, n_head: the number of heads (that we would like to use)\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "# Very simple bigram (language) model (We can go for a trigram model or more, from here) This is just to simplify the process as much as possible for now:\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # lookup table, where each token directly reads off the logits for the next token, from it\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # last norm layer (LayerNorm)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets : tensor of integers, they are both (B,T)\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) || device in case of converting a CPU Tensor with pinned memory to a CUDA Tensor !\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size) not necessarily the same as C\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "\n",
        "m = model.to(device)\n",
        "# Display/print the number of parameters of our model here:\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # numel === number elements : number of elements (in total) in the tensor !!\n",
        "\n",
        "#Optimizer (PyTorch)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # evaluating the loss on our sets for train and val, sometimes\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Step {iter}: training loss {losses['train']:.4f}, validation loss {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "  # a sampled batch of data:\n",
        "  xb, yb = get_batch('train')\n",
        "  #evaluation of the loss:\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Using the compiled + trained model to generate max_new_tokens = 1337 :\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode((m.generate(context, max_new_tokens=1337))[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP5fh8i-jFau",
        "outputId": "684394b4-3dcc-4cdd-b444-2f1d4ba2aee8"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "Step 0: training loss 4.4077, validation loss 4.4038\n",
            "Step 120: training loss 2.6161, validation loss 2.6279\n",
            "Step 240: training loss 2.4616, validation loss 2.4685\n",
            "Step 360: training loss 2.3656, validation loss 2.3954\n",
            "Step 480: training loss 2.3000, validation loss 2.3199\n",
            "Step 600: training loss 2.2399, validation loss 2.2606\n",
            "Step 720: training loss 2.1885, validation loss 2.2079\n",
            "Step 840: training loss 2.1383, validation loss 2.1777\n",
            "Step 960: training loss 2.1065, validation loss 2.1566\n",
            "Step 1080: training loss 2.0736, validation loss 2.1254\n",
            "Step 1200: training loss 2.0255, validation loss 2.0916\n",
            "Step 1320: training loss 2.0077, validation loss 2.0815\n",
            "Step 1440: training loss 1.9847, validation loss 2.0627\n",
            "Step 1560: training loss 1.9636, validation loss 2.0494\n",
            "Step 1680: training loss 1.9352, validation loss 2.0253\n",
            "Step 1800: training loss 1.9215, validation loss 2.0391\n",
            "Step 1920: training loss 1.8925, validation loss 2.0102\n",
            "Step 2040: training loss 1.8829, validation loss 1.9947\n",
            "Step 2160: training loss 1.8722, validation loss 2.0079\n",
            "Step 2280: training loss 1.8612, validation loss 1.9957\n",
            "Step 2400: training loss 1.8358, validation loss 1.9804\n",
            "Step 2520: training loss 1.8171, validation loss 1.9701\n",
            "Step 2640: training loss 1.8171, validation loss 1.9732\n",
            "Step 2760: training loss 1.7989, validation loss 1.9661\n",
            "Step 2880: training loss 1.7941, validation loss 1.9717\n",
            "Step 3000: training loss 1.7588, validation loss 1.9617\n",
            "Step 3120: training loss 1.7822, validation loss 1.9558\n",
            "Step 3240: training loss 1.7571, validation loss 1.9567\n",
            "Step 3360: training loss 1.7645, validation loss 1.9664\n",
            "Step 3480: training loss 1.7439, validation loss 1.9241\n",
            "Step 3600: training loss 1.7333, validation loss 1.9277\n",
            "Step 3720: training loss 1.7347, validation loss 1.9443\n",
            "Step 3840: training loss 1.7349, validation loss 1.9307\n",
            "Step 3960: training loss 1.7391, validation loss 1.9311\n",
            "Step 4080: training loss 1.7129, validation loss 1.9386\n",
            "Step 4200: training loss 1.7097, validation loss 1.9299\n",
            "Step 4320: training loss 1.6968, validation loss 1.9259\n",
            "Step 4440: training loss 1.6989, validation loss 1.9192\n",
            "Step 4560: training loss 1.6868, validation loss 1.9187\n",
            "Step 4680: training loss 1.6876, validation loss 1.9126\n",
            "Step 4800: training loss 1.6665, validation loss 1.9011\n",
            "Step 4920: training loss 1.6759, validation loss 1.9045\n",
            "Step 5040: training loss 1.6677, validation loss 1.8946\n",
            "Step 5160: training loss 1.6711, validation loss 1.9233\n",
            "Step 5280: training loss 1.6616, validation loss 1.9099\n",
            "Step 5400: training loss 1.6688, validation loss 1.9037\n",
            "Step 5520: training loss 1.6501, validation loss 1.8863\n",
            "Step 5640: training loss 1.6501, validation loss 1.9064\n",
            "Step 5760: training loss 1.6481, validation loss 1.8907\n",
            "Step 5880: training loss 1.6397, validation loss 1.9114\n",
            "Step 6000: training loss 1.6452, validation loss 1.8978\n",
            "Step 6120: training loss 1.6406, validation loss 1.8865\n",
            "Step 6149: training loss 1.6328, validation loss 1.8942\n",
            "\n",
            "LORD GAURET:\n",
            "I comfory, when show you have so doth is\n",
            "I hole chelf. I, cariol'd old, have me remsely, live my\n",
            "Papperace: myserrable hause\n",
            "You part banity marry my loncerand redirence is restliist thinks the crown\n",
            "Of gome a my lord Coriolo,\n",
            "To we assely lozed any mine her?\n",
            "\n",
            "POLINTES:\n",
            "Thoughasom distrer gold, think; of it this anperate\n",
            "Chall more deving slike their is made thans theat,\n",
            "Will and my lord,--las have some you, prosroness;\n",
            "Though wash mainsters joyfocion, hese, in of they to feeth\n",
            "As lord all brisake\n",
            "affore distretch ovengered; it as wong,\n",
            "And then the hear own childom, of hese is have will drumptricident:\n",
            "Have she thou ship his these aemh:\n",
            "shapt yet fock et ruke baught, now'd Dothers a oward, but a whereous her hand\n",
            "That the to-from her propline.\n",
            "\n",
            "ICABPLLA:\n",
            "All; thou defited gentomend him ounders no head on of 'tis gende.\n",
            "\n",
            "ISABELLA:\n",
            "I amake Cliffords, he sir, thus is daways\n",
            "And, the artitence; seecy wrong;\n",
            "That comeas more condent therefore coth.\n",
            "\n",
            "BELTHOKE OWY:\n",
            "A my like my thee the sady dast?\n",
            "\n",
            "KING RICHARD II KINLI:\n",
            "Then he steeds of as teo\n",
            "Youless? 'Tis have own this thee!\n",
            "'tis have a mean this of mercy had and the lambrawd!\n",
            "Could thy mighan 'ill not near a brot\n",
            "Medger see thee embern's was; 'tis you, then,\n",
            "Ye'll evence, this shall which a we shall me not o's of her.\n",
            "\n",
            "Serron thou minest it foul the shar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BETmBvPt-_Q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}